{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv(\"Train_values.csv\")\n",
    "train_labels=pd.read_csv(\"Train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_values.merge(train_labels,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('Test_values.csv')\n",
    "test_copy=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert status group label into numerical data\n",
    "val_status_group={'functional':2, 'functional needs repair':1,\n",
    "                   'non functional':0}\n",
    "train['status_group_vals']=train.status_group.replace(val_status_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on each field one by one\n",
    "#funder\n",
    "train['funder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets divide funder column into 5 categories, top 5 will be remained as it is and after that they will be categorised into other\n",
    "def funder_cleaning(train):\n",
    "    if train['funder']=='Government Of Tanzania':\n",
    "        return 'government'\n",
    "    elif train['funder']=='Danida':\n",
    "        return 'danida'\n",
    "    elif train['funder']=='Hesawa':\n",
    "        return 'hesawa'\n",
    "    elif train['funder']=='Rwssp':\n",
    "        return 'rwssp'\n",
    "    elif train['funder']=='World Bank':\n",
    "        return 'world_bank'    \n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "train['funder']= train.apply(lambda row: funder_cleaning(row), axis=1)\n",
    "test['funder']= test.apply(lambda row: funder_cleaning(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on each field one by one\n",
    "#installer\n",
    "train['installer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets divide installer column into 5 categories, top 5 will be remained as it is and after that they will be categorised into other\n",
    "def installer_cleaning(train):\n",
    "    if train['installer']=='DWE':\n",
    "        return 'dwe'\n",
    "    elif train['installer']=='Government':\n",
    "        return 'government'\n",
    "    elif train['installer']=='RWE':\n",
    "        return 'rwe'\n",
    "    elif train['installer']=='Commu':\n",
    "        return 'commu'\n",
    "    elif train['installer']=='DANIDA':\n",
    "        return 'danida'    \n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "train['installer']= train.apply(lambda row: installer_cleaning(row), axis=1)\n",
    "test['installer']= test.apply(lambda row: installer_cleaning(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "# Checking Null Values on training data\n",
    "train.apply(lambda x: sum(x.isnull()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_table  = pd.pivot_table(train,index=['funder','status_group'],\n",
    "                           values='status_group_vals', aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like funder and installer it is hard to categorize them into 5 or 6 subvillages because values are not dominating \n",
    "#and 19287 unique values are there and the top values are not dominating though\n",
    "\n",
    "train['subvillage'].value_counts()\n",
    "\n",
    "#better to drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(['subvillage'],axis=1)\n",
    "test=test.drop(['subvillage'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_meeting\n",
    "train['public_meeting'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since most of the values are True, as of now lets insert True for the missing values. Scope to alter the values in future\n",
    "train.public_meeting = train.public_meeting.fillna('Unknown')\n",
    "test.public_meeting = test.public_meeting.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheme Management\n",
    "train['scheme_management'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to reduce the amount of dummy columns needed whilst maintaining the \n",
    "# information contained in the column.\n",
    "\n",
    "def scheme_wrangler(row):\n",
    "    '''Keep top 5 values and set the rest to 'other'. '''\n",
    "    if row['scheme_management']=='VWC':\n",
    "        return 'vwc'\n",
    "    elif row['scheme_management']=='WUG':\n",
    "        return 'wug'\n",
    "    elif row['scheme_management']=='Water authority':\n",
    "        return 'wtr_auth'\n",
    "    elif row['scheme_management']=='WUA':\n",
    "        return 'wua'\n",
    "    elif row['scheme_management']=='Water Board':\n",
    "        return 'wtr_brd'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "train['scheme_management'] = train.apply(lambda row: scheme_wrangler(row), axis=1)\n",
    "test['scheme_management'] = test.apply(lambda row: scheme_wrangler(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheme name\n",
    "train['scheme_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.scheme_name.unique())\n",
    "\n",
    "# Lots of factors and the top 5 or so only represent a fraction of the total values. Probably \n",
    "# safe to drop this column.\n",
    "\n",
    "train = train.drop('scheme_name', axis=1)\n",
    "test = test.drop('scheme_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permit\n",
    "train['permit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only have two values here: true and false. This one can stay but we'll have to replace \n",
    "# the unknown data with a string value.\n",
    "\n",
    "train.permit = train.permit.fillna('Unknown')\n",
    "test.permit = test.permit.fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "# Checking Null Values on test data\n",
    "train.apply(lambda x: sum(x.isnull()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above data, we can say the data is clean for both the training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the correlation values of training datatest.corr()\n",
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the correlation values of testing datatest.corr()\n",
    "test.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values are not correlated with each other. That is good for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['recorded_by'].value_counts()\n",
    "#we can drop this column because all the values in this column are same. There is no point to go forward with this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(['recorded_by'],axis=1)\n",
    "test=test.drop(['recorded_by'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['waterpoint_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['waterpoint_type_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterpoint_type and waterpoint_type_group both are almost similar, communal standpipe and comunal standpipe multiple are merged \n",
    "# together in waterpoint_type_group\n",
    "# we can drop one of them\n",
    "\n",
    "train=train.drop(['waterpoint_type'],axis=1)\n",
    "test=test.drop(['waterpoint_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['source_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above script, we can say the source and source_type are same because in source_type some values are merged together. \n",
    "# we can drop one of the column.\n",
    "\n",
    "train=train.drop(['source'],axis=1)\n",
    "test=test.drop(['source'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quantity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quantity_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above script, we can say the quantity and quantity_group both are same.\n",
    "# we can drop one of the column.\n",
    "\n",
    "train=train.drop(['quantity'],axis=1)\n",
    "test=test.drop(['quantity'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['water_quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quality_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#water_quality and quality_group are correlated . lets drop one of the column\n",
    "train=train.drop(['quality_group'],axis=1)\n",
    "test=test.drop(['quality_group'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['payment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['payment_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above script, we can say the payment and payment_type both are same.\n",
    "# we can drop one of the column.\n",
    "\n",
    "train=train.drop(['payment'],axis=1)\n",
    "test=test.drop(['payment'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['management'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['management_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above script, we can say the management and management_group both are same, some values are merged together in management-group.\n",
    "# we can drop one of the column.\n",
    "\n",
    "train=train.drop(['management'],axis=1)\n",
    "test=test.drop(['management'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['extraction_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['extraction_type_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['extraction_type_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above script, we can say the exrtaction_type, extraction_type_group and extraction_type_class are same, some values are merged together.\n",
    "\n",
    "train=train.drop(['extraction_type'],axis=1)\n",
    "test=test.drop(['extraction_type'],axis=1)\n",
    "\n",
    "train=train.drop(['extraction_type_group'],axis=1)\n",
    "test=test.drop(['extraction_type_group'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gps_height, longitude, latitude, region_code and district_code are all geographic info which# gps_he \n",
    "# is unlikely to add any predictive power to the model given that there are other variables\n",
    "# containing geographic data. 'num_private' hasn't been given a discription on Driven Data,\n",
    "# it appears to be superflous. We expect id to not contain any useful information so that gets\n",
    "# dropped too. wpt_name is also not required, it gives only waterpoint name\n",
    "\n",
    "train = train.drop(['gps_height', 'longitude', 'latitude', 'region_code', 'district_code',\n",
    "             'num_private', 'id','wpt_name','lga','region','ward','status_group'], axis=1)\n",
    "\n",
    "test = test.drop(['gps_height', 'longitude', 'latitude', 'region_code', 'district_code',\n",
    "             'num_private', 'id','wpt_name','lga','region','ward'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn construction_year into a categorical column containing the following values: '60s', '70s',\n",
    "# '80s', '90s, '00s', '10s', 'unknown'.\n",
    "\n",
    "def construction_wrangler(row):\n",
    "    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:\n",
    "        return '60s'\n",
    "    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:\n",
    "        return '70s'\n",
    "    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:\n",
    "        return '80s'\n",
    "    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:\n",
    "        return '90s'\n",
    "    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:\n",
    "        return '00s'\n",
    "    elif row['construction_year'] >= 2010:\n",
    "        return '10s'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "train['construction_year'] = train.apply(lambda row: construction_wrangler(row), axis=1)\n",
    "test['construction_year'] = test.apply(lambda row: construction_wrangler(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['population'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.date_recorded = pd.datetime(2013, 12, 3) - pd.to_datetime(test.date_recorded)\n",
    "test.columns = ['days_since_recorded' if x=='date_recorded' else x for x in test.columns]\n",
    "test.days_since_recorded = test.days_since_recorded.astype('timedelta64[D]').astype(int)\n",
    "\n",
    "train.date_recorded = pd.datetime(2013, 12, 3) - pd.to_datetime(train.date_recorded)\n",
    "train.columns = ['days_since_recorded' if x=='date_recorded' else x for x in train.columns]\n",
    "train.days_since_recorded = train.days_since_recorded.astype('timedelta64[D]').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy columns for the categorical columns and shuffle the data.\n",
    "\n",
    "dummy_cols = ['funder', 'installer', 'basin', 'public_meeting', 'scheme_management', 'permit',\n",
    "              'construction_year', 'extraction_type_class','management_group', 'payment_type', 'water_quality',\n",
    "              'quantity_group', 'source_type', 'source_class','waterpoint_type_group']\n",
    "\n",
    "train = pd.get_dummies(train, columns = dummy_cols)\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns = dummy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of training data',train.shape)\n",
    "print('Shape of testing data',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiling report of training data\n",
    "pandas_profiling.ProfileReport(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from profiling report, we can drop duplicate rows from the training dataset and also we can see \n",
    "#waterpoint_type_group_hand pump is highly correlated with extraction_type_class_handpump. we can drop any one of the column\n",
    "\n",
    "#train=train.drop_duplicates()\n",
    "\n",
    "train=train.drop(['waterpoint_type_group_hand pump'],axis=1)\n",
    "test=test.drop(['waterpoint_type_group_hand pump'],axis=1)\n",
    "\n",
    "train=train.drop(['source_type_other '],axis=1)\n",
    "test=test.drop(['source_type_other '],axis=1)\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the train set into train and validation sets. Also remove the target.\n",
    "\n",
    "target = train.status_group_vals\n",
    "features = train.drop('status_group_vals', axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, target, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_svc_model(X_train, X_val, y_train, y_val, test):\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        #scl = StandardScaler()\n",
    "        clf = LinearSVC()\n",
    "        \n",
    "        parameters = {'C':[0.001,0.01,0.1,1.0,10.0,100.0],'class_weight':[None, 'balanced']}\n",
    "\n",
    "        estimator = GridSearchCV(clf, parameters,n_jobs=-1)\n",
    "\n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        best_params = estimator.best_params_\n",
    "                                 \n",
    "        validation_accuracy = estimator.score(X_val, y_val)\n",
    "        print('Validation accuracy: ', validation_accuracy)\n",
    "        print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_svc_model(X_train, X_val, y_train, y_val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoostingClassifier_model(X_train, X_val, y_train, y_val, test):\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        \n",
    "        gb = GradientBoostingClassifier()\n",
    "        \n",
    "        parameters = {'learning_rate': [0.7],\n",
    "                      'max_depth': [14],\n",
    "                      'min_samples_leaf': [16],\n",
    "                      'max_features': [1.0],\n",
    "                      'n_estimators': [200]}\n",
    "        \n",
    "        estimator = GridSearchCV(gb,n_jobs=-1)\n",
    "        #parameters = {'learning_rate': [0.7],\n",
    "        #              'max_depth': [14],\n",
    "        #              'min_samples_leaf': [16],\n",
    "        #              'max_features': [1.0],\n",
    "        #              'n_estimators': [200]}\n",
    "        \n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        best_params = estimator.best_params_\n",
    "                                 \n",
    "        validation_accuracy = estimator.score(X_val, y_val)\n",
    "        print('Validation accuracy: ', validation_accuracy)\n",
    "        print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier_model(X_train, X_val, y_train, y_val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pd.DataFrame\n",
    "test_id=test_copy['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_submission(features, target, test):\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        gb = GradientBoostingClassifier()\n",
    "        \n",
    "        #parameters = {'learning_rate': [0.7],\n",
    "        #              'max_depth': [14],\n",
    "        #              'min_samples_leaf': [16],\n",
    "        #              'max_features': [1.0],\n",
    "        #              'n_estimators': [200]}\n",
    "        \n",
    "        parameters = {'learning_rate': [0.7],'max_features': [1.0],'min_samples_leaf': [16],'max_depth': [14]}\n",
    "\n",
    "        estimator = GridSearchCV(gb, parameters,n_jobs=-1)\n",
    "\n",
    "        estimator.fit(features, target)        \n",
    "\n",
    "        predictions = estimator.predict(test)\n",
    "\n",
    "        data = {'ID': test_id, 'status_group': predictions}\n",
    "\n",
    "        submit = pd.DataFrame(data=data)\n",
    "\n",
    "        vals_to_replace = {2:'functional', 1:'functional needs repair',\n",
    "                           0:'non functional'}\n",
    "\n",
    "        submit.status_group = submit.status_group.replace(vals_to_replace)        \n",
    "\n",
    "        submit.to_csv('pump_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model for submission.\n",
    "model_for_submission(features, target, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
